{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-father",
   "metadata": {
    "executionInfo": {
     "elapsed": 3396,
     "status": "ok",
     "timestamp": 1621021311508,
     "user": {
      "displayName": "TYE Mfg Software",
      "photoUrl": "",
      "userId": "07235769425301603693"
     },
     "user_tz": 240
    },
    "id": "l8XGDecVogxG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve                # Calculate the ROC curve\n",
    "from sklearn.metrics import precision_recall_curve   # Calculate the Precision-Recall curve\n",
    "from sklearn.metrics import f1_score                 # Calculate the F-score\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os \n",
    "import time\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPool1D, Dropout,Flatten,Input,concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import shutil\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-consideration",
   "metadata": {
    "id": "1b3NGjg7pwda"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-receiver",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 5292,
     "status": "ok",
     "timestamp": 1621021319183,
     "user": {
      "displayName": "TYE Mfg Software",
      "photoUrl": "",
      "userId": "07235769425301603693"
     },
     "user_tz": 240
    },
    "id": "stable-mention",
    "outputId": "881867b7-4105-4a0e-ce6e-29b830d73016"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "THRESHOLD = 15\n",
    "LABEL_DATA= r''\n",
    "REG_DATA= r''\n",
    "COST_DATA = r''\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "df=pd.read_csv(LABEL_DATA)\n",
    "df_regr =pd.read_csv(REG_DATA)\n",
    "\n",
    "#shuffle the dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.dropna(subset = [\"Description\"], inplace=True)\n",
    "\n",
    "\n",
    "#convert nan to 0\n",
    "df_regr=df_regr.fillna(0)\n",
    "\n",
    "df_short_desc=df['Short_Description']\n",
    "df_regr.drop(['Description'],axis=1,inplace=True)\n",
    "\n",
    "#merge to ensure the our datasets have matching entries. \n",
    "df_regr = pd.merge(df[['InvtID','Description']],\n",
    "                 df_regr,\n",
    "                 on='InvtID', \n",
    "                 how='left')\n",
    "\n",
    "#data not needed for this task. \n",
    "df_regr.drop(['ID','InvtID','Date', 'Code','Job_Number','Year'],axis=1,inplace=True)\n",
    "df.drop(['InvtID'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "description_category = df\n",
    "description_category['Description']= description_category['Description'].astype(str)\n",
    "freq_plot = pd.DataFrame()\n",
    "freq_plot['cat'] = description_category.columns[1:]\n",
    "freq_plot['count'] = description_category.iloc[:,1:].sum().values\n",
    "freq_plot.sort_values(['count'], inplace=True, ascending=False)\n",
    "freq_plot.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "main_categories = pd.DataFrame()\n",
    "print(freq_plot.shape)\n",
    "#we remove components below a certain threshold as they are rarely used.\n",
    "main_categories = freq_plot[(freq_plot['count']>THRESHOLD)]\n",
    "print(main_categories.shape)\n",
    "#categories has a list of cats above threshold \n",
    "categories = main_categories['cat'].values\n",
    "\n",
    "not_category = []\n",
    "# description_category['Hardware'] = 0\n",
    "\n",
    "#iterate through list of columns and add counts for condesned group\n",
    "for i in description_category.columns[1:]:\n",
    "    if i not in categories:\n",
    "        #description_category['Hardware'][description_category[i] == 1] = 1\n",
    "        not_category.append(i)\n",
    "\n",
    "description_category.drop(not_category, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "most_common_cat = pd.DataFrame()\n",
    "most_common_cat['cat'] = description_category.columns[1:]\n",
    "most_common_cat['count'] = description_category.iloc[:,1:].sum().values\n",
    "most_common_cat.sort_values(['count'], inplace=True, ascending=False)\n",
    "most_common_cat.reset_index(inplace=True, drop=True)\n",
    "print(most_common_cat.tail(50))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(24,8))\n",
    "sns.set(font_scale = 1.5)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "\n",
    "\n",
    "pal = sns.color_palette(\"flare\", len(most_common_cat))\n",
    "rank = most_common_cat['count'].argsort().argsort()\n",
    "\n",
    "sns.barplot(most_common_cat['cat'], most_common_cat['count'], palette=np.array(pal[::-1])[rank])\n",
    "plt.axhline(THRESHOLD, ls='--', c='red')\n",
    "plt.title(\"Component frequency\", fontsize=24)\n",
    "plt.ylabel('Number of jobs', fontsize=18)\n",
    "plt.xlabel('components', fontsize=18)\n",
    "plt.xticks(rotation='vertical',size = 5)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "executed-station",
   "metadata": {
    "id": "painful-pleasure"
   },
   "source": [
    "# Data manipulation and cleanining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-fantasy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5425,
     "status": "ok",
     "timestamp": 1621021320626,
     "user": {
      "displayName": "TYE Mfg Software",
      "photoUrl": "",
      "userId": "07235769425301603693"
     },
     "user_tz": 240
    },
    "id": "senior-spice",
    "outputId": "881a9df9-11b2-4cb5-c731-1a61c49b83c8"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Scale regression values\n",
    "df_regr = df_regr[df.columns[1:]]\n",
    "print(df_regr.describe())\n",
    "x = df_regr.values #returns a numpy array\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "x_scaled = standard_scaler.fit_transform(x)\n",
    "df_regr = pd.DataFrame(x_scaled,columns= df_regr.columns)\n",
    "print(df_regr.describe())\n",
    "print(df_regr.head(20))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters this code could be optimized\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#|_|\\*|{|}|##|`|~|@|:]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/|\\n|\\t|\\r|]',r' ',cleaned)\n",
    "    cleaned = re.sub(' +', ' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"  \",\" \")\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n",
    "description_category['Description'] = description_category['Description'].apply(cleanPunc)\n",
    "df_short_desc=df_short_desc.apply(cleanPunc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-capital",
   "metadata": {
    "id": "TsSYb3h_OrjQ"
   },
   "source": [
    "# TF-IDF and Tokenization models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-biodiversity",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1621021323370,
     "user": {
      "displayName": "TYE Mfg Software",
      "photoUrl": "",
      "userId": "07235769425301603693"
     },
     "user_tz": 240
    },
    "id": "adjacent-government",
    "outputId": "5c95c0f7-f4fd-40c2-c234-b2aff48b0664",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def tfidf_features(X_train):\n",
    "\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True,\n",
    "                                       ngram_range=(1,3),\n",
    "                                       analyzer='word',\n",
    "                                       stop_words='english',\n",
    "                                        max_features = 4995\n",
    "                                           \n",
    "                                       )\n",
    "    \n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)  \n",
    "    return X_train_tfidf.toarray(), tfidf_vectorizer.vocabulary_ ,tfidf_vectorizer\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2855, lower=True)\n",
    "tokenizer.fit_on_texts(df_short_desc)\n",
    "tokenized_short_description = tokenizer.texts_to_sequences(df_short_desc)\n",
    "long_description = description_category['Description'].to_list()\n",
    "max_len_short_description=1200\n",
    "x =long_description\n",
    "x2 = pad_sequences(tokenized_short_description, maxlen=max_len_short_description)\n",
    "\n",
    "\n",
    "\n",
    "seeds = [1, 42, 79, 121, 172]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = x\n",
    "X2_train = x2\n",
    "y_train = df_regr\n",
    "\n",
    "\n",
    "\n",
    "def format_and_reshape(X2_train,X2_test,max_len_short_description):\n",
    "    X2_train= [l.tolist() for l in X2_train]\n",
    "    X2_train=np.concatenate( X2_train, axis=0 )\n",
    "    X2_train=X2_train.reshape(-1,max_len_short_description)\n",
    "\n",
    "    X2_test= [l.tolist() for l in X2_test]\n",
    "    X2_test=np.concatenate( X2_test, axis=0 )\n",
    "    X2_test=X2_test.reshape(-1,max_len_short_description)\n",
    "    return X2_train,X2_test\n",
    "\n",
    "\n",
    "X_train, tfidf_vocab, tfidf_transformer = tfidf_features(X_train)\n",
    "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}\n",
    "\n",
    "\n",
    " \n",
    "max_words_short_desc=len(tokenizer.word_index) + 1\n",
    "num_classes = y_train.shape[1]\n",
    "max_words_long_desc = len(tfidf_vocab)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-expense",
   "metadata": {
    "id": "bx9w9dzHPsWx"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-fusion",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 14898,
     "status": "error",
     "timestamp": 1621020897931,
     "user": {
      "displayName": "TYE Mfg Software",
      "photoUrl": "",
      "userId": "07235769425301603693"
     },
     "user_tz": 240
    },
    "id": "oeSTP2RxP2gS",
    "outputId": "1d02bfdd-0ad7-4ac2-933c-22a19dffaa6f"
   },
   "outputs": [],
   "source": [
    "#determine parameters for regression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "# tensorboard_cb,\n",
    "ReduceLROnPlateau(),\n",
    "EarlyStopping(patience=10),\n",
    "ModelCheckpoint(filepath=r'{}-model-simple.h5'.format(\"reg_opt\"), save_best_only=True)\n",
    "]\n",
    "\n",
    "\n",
    "def custom_mse(class_weights):\n",
    "    def cost_loss(y_true, y_pred):\n",
    "        diff = (y_pred - y_true) * class_weights\n",
    "        mse = K.mean(K.abs(diff), axis=-1)\n",
    " \n",
    "        return mse\n",
    "    return cost_loss\n",
    "\n",
    "\n",
    "\n",
    "def custom_metric(class_weights):\n",
    "    def cost_loss(y_true, y_pred):\n",
    "        diff = K.abs(tf.math.subtract(y_pred,y_true))\n",
    "        diff = tf.math.multiply(diff,class_weights)\n",
    " \n",
    "        print(class_weights)\n",
    "        true_cost= K.sum(K.abs(tf.math.multiply(y_true,class_weights)))\n",
    "#         percent_cost_error= tf.math.divide_no_nan(diff,K.abs(y_true))\n",
    "        mse = K.sum(diff, axis=-1)\n",
    "        mse = tf.math.divide_no_nan(mse,true_cost)\n",
    " \n",
    "        return mse\n",
    "    return cost_loss\n",
    "\n",
    "\n",
    "\n",
    "df_costs=df_costs[categories]\n",
    "\n",
    "cost_scaler= MinMaxScaler(feature_range=(0,10))\n",
    "cost_arr = cost_scaler.fit_transform(df_costs.to_numpy().reshape(-1,1) )\n",
    "print(cost_arr.shape)\n",
    "print(cost_arr)\n",
    "\n",
    "list1 = cost_arr.flatten().tolist()\n",
    "print(list1)\n",
    "class_weights_1 = K.variable(list1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_costs=pd.read_csv(COST_DATA)\n",
    "df_costs=df_costs[categories]\n",
    "minimum= min(df_costs.values.tolist()[0])\n",
    "maximum= max(df_costs.values.tolist()[0])\n",
    "cost_scaler= MinMaxScaler(feature_range=(minimum,maximum))\n",
    "cost_arr = cost_scaler.fit_transform(df_costs.to_numpy().reshape(-1,1) )\n",
    "print(cost_arr.shape)\n",
    "print(cost_arr)\n",
    "\n",
    "list1 = cost_arr.flatten().tolist()\n",
    "print(list1)\n",
    "class_weights_2 = K.variable(list1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "target_metric= 'cost_loss'\n",
    "target_metric = 'cost_loss'\n",
    "#or cost_loss\n",
    "def build_model_regression_only(hp):\n",
    "\n",
    " \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-1,1e-2,1e-3, 1e-4,1e-5,1e-6])\n",
    "    hp_l1 = hp.Choice('l1', values=[1e-3, 1e-4, 1e-5])\n",
    "    hp_l2 = hp.Choice('l2', values=[1e-3, 1e-4, 1e-5])\n",
    "    dropout = hp.Choice('dr', values=[0.1,0.2])\n",
    "    dropout_2 = hp.Choice('dr_2', values=[0.0,0.1,0.2])\n",
    "    number_of_layers = hp.Int('num_layers',min_value= 1, max_value=10)\n",
    "\n",
    "    hp_units = hp.Int('units', min_value=1024, max_value=4096, step=64)\n",
    "    hp_units_2 = hp.Int('units_2', min_value=184, max_value=512, step=64)\n",
    "    \n",
    "\n",
    "\n",
    "    input1 = Input(shape=(max_len_short_description,),name='short_desc')\n",
    "    input2 = Input(shape=(max_words_long_desc,),name='long_desc')\n",
    "\n",
    "\n",
    "    y2 = Dense(int(max_words_long_desc), input_shape=(max_words_long_desc,),kernel_initializer='he_normal')(input2)\n",
    "    y2 = Dense(int(max_words_long_desc),kernel_initializer='he_normal')(y2)\n",
    "    y2 = Dense(int(max_words_long_desc),kernel_initializer='he_normal')(y2)\n",
    "\n",
    "    y1= Dense(hp_units_2, activation='relu',kernel_initializer='he_normal')(input1)\n",
    "    y1= Dense(hp_units_2, activation='relu',kernel_initializer='he_normal')(y1)\n",
    "    y1= Dense(hp_units_2, activation='relu',kernel_initializer='he_normal')(y1)\n",
    "\n",
    "    y = concatenate([y1, y2])\n",
    "\n",
    "\n",
    "    for i in range(number_of_layers):\n",
    "        y= Dense(hp_units, activation='relu',kernel_initializer='he_normal',kernel_regularizer=tf.keras.regularizers.L1L2(l1=hp_l1,l2=hp_l2),name=\"Dense_{}\".format(i+100))(y)\n",
    "        y=Dropout(dropout_2)(y)\n",
    "    y=Dropout(dropout)(y)\n",
    "    y = Dense(num_classes,activation='linear',name=\"Dense_linear_{}\".format(300))(y)\n",
    "    model = Model(inputs=[input1, input2], outputs=y)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), \n",
    "                loss= custom_mse(class_weights_1),\n",
    "                metrics=[custom_metric(class_weights_2)]\n",
    "                )\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tuner_reg = kt.Hyperband(build_model_regression_only,\n",
    "                     objective=kt.Objective(target_metric, direction=\"min\"),\n",
    "                     max_epochs=10,\n",
    "                     directory='my_dir_3',\n",
    "                     project_name='intro_to_kt_3')\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor=target_metric, patience=5)\n",
    "tuner_reg.search([X2_train,X_train], y_train, \n",
    "             epochs=50, \n",
    "             validation_split=0.2, \n",
    "             callbacks=[stop_early]\n",
    "#              class_weight=class_weight\n",
    "                )\n",
    "\n",
    "\n",
    "best_hps=tuner_reg.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')} \n",
    "l1 {best_hps.get('l1')} \n",
    "l2 {best_hps.get('l2')} \n",
    "dr {best_hps.get('dr')}\n",
    "dr_2 {best_hps.get('dr_2')}\n",
    "units_2 {best_hps.get('units_2')}\n",
    "num_layers {best_hps.get('num_layers')}\n",
    "\n",
    ".\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-marks",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#CV evaluation \n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "zip_cv = list(zip(X_train,X2_train))\n",
    "print(len(zip_cv))\n",
    "\n",
    "fold_no = 1\n",
    "average_score_reg_only=[]\n",
    "for train, test in kfold.split(zip_cv, y_train):\n",
    "\n",
    "\n",
    "    X2_train_cv= X2_train[train]\n",
    "    X_train_cv = X_train[train]\n",
    "    Y_train_cv = y_train.to_numpy()[train]\n",
    "    x_values = [X2_train[train],X_train[train]]\n",
    "    model_reg_only = tuner_reg.hypermodel.build(best_hps)\n",
    "    plot_model(model_reg_only, to_file='model.png')\n",
    "    history_reg_only = model_reg_only.fit([X2_train_cv,X_train_cv],Y_train_cv, epochs=50,verbose=0)\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    val_acc_per_epoch = history_reg_only.history[target_metric]\n",
    "    best_epoch = val_acc_per_epoch.index(min(val_acc_per_epoch)) + 1\n",
    "    print('Best epoch: %d' % (best_epoch,))\n",
    "    callbacks = [\n",
    "    ReduceLROnPlateau(monitor=target_metric),\n",
    "    EarlyStopping(patience=10,monitor=target_metric),\n",
    "    ModelCheckpoint(filepath=r'{}-model-simple.h5'.format(\"sig_opt_final\"),monitor=target_metric, save_best_only=True)\n",
    "    ]\n",
    "    hypermodel = tuner_reg.hypermodel.build(best_hps)\n",
    "    history = hypermodel.fit([X2_train_cv,X_train_cv], Y_train_cv, epochs=best_epoch, callbacks=callbacks,verbose=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Generate generalization metrics\n",
    "    scores = hypermodel.evaluate([X2_train[test],X_train[test]], y_train.to_numpy()[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model_reg_only.metrics_names[0]} of {scores[0]}; {model_reg_only.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    average_score_reg_only.append(scores[1]*100)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    acc = history.history[target_metric]\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'b', label='Training MAE')\n",
    "    plt.title('Training WMAE')\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "\n",
    "print(Average(average_score_reg_only))  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1b3NGjg7pwda",
    "painful-pleasure",
    "TsSYb3h_OrjQ",
    "jWhU2HShO_GM",
    "1-D4hpSqlFLy",
    "controversial-daughter"
   ],
   "name": "multi_label_classification_sigmoid_optimization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu-cuda8]",
   "language": "python",
   "name": "conda-env-tf-gpu-cuda8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
